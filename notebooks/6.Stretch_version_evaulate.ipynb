{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba4fd975",
   "metadata": {},
   "source": [
    "# Evaulate A Reduced LLAMA3.2-1B Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca56e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsloth/Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import DataCollatorForSeq2Seq, EarlyStoppingCallback, AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "print(MODEL_NAME)\n",
    "# Training settings\n",
    "# llama3.2 support 128k tokens\n",
    "MAX_SEQ_LENGTH = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "LOAD_IN_4BIT = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "WANDB_PROJECT = \"BMW-Llama-3.2-1B\"\n",
    "WANDB_ENTITY = None  \n",
    "WANDB_RUN_NAME = \"BMW-Llama-3.2-1B-2000Articles_pruned\"\n",
    "\n",
    "TRAIN_CHAT_DATA_FILE_NAME = '../datasets/chat_data_2000/train_chat.jsonl'\n",
    "VAL_CHAT_DATA_FILE_NAME = '../datasets/chat_data_2000/val_chat.jsonl'\n",
    "TEST_CHAT_DATA_FILE_NAME = '../datasets/chat_data_2000/test_chat.jsonl'\n",
    "\n",
    "# Set checkpoint directory\n",
    "CHECKPOINT_DIR = f\"../{WANDB_RUN_NAME}\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Use the checkpoint_dir variable to save models\n",
    "LORA_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"lora_model\")\n",
    "MERGED_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"merged_model\")\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(LORA_MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(MERGED_MODEL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fedc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded final test dataset: 444 samples\n",
      "Features: {'messages': List({'role': Value('string'), 'content': Value('string')})}\n"
     ]
    }
   ],
   "source": [
    "dataset_test = load_dataset(\"json\", data_files=TEST_CHAT_DATA_FILE_NAME, split=\"train\")\n",
    "test_ds = Dataset.from_list(dataset_test)\n",
    "\n",
    "print(\"HuggingFace Chat Datasets:\")\n",
    "print(f\"  Test: {test_ds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e630dbbf",
   "metadata": {},
   "source": [
    "# Test Model\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "988b0dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-14): 15 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    dtype=torch.bfloat16, \n",
    "    device_map='auto'\n",
    "    )\n",
    "base_model.model.layers = torch.nn.ModuleList(base_model.model.layers[:-1])\n",
    "base_model.config.num_hidden_layers = len(base_model.model.layers)\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "406b3f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '../bmw-llama-3.2-1b-2000articles_stretch_lessLayer/merged_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0-14): 15 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = AutoModelForCausalLM.from_pretrained(MERGED_MODEL_PATH, dtype=torch.bfloat16, device_map='auto')\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH)\n",
    "\n",
    "# Ensure pad token is set (in case it wasn't saved properly)\n",
    "if test_tokenizer.pad_token is None:\n",
    "    test_tokenizer.pad_token = test_tokenizer.eos_token\n",
    "\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726c9cd",
   "metadata": {},
   "source": [
    "# Evaluate Model Performance\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100084a",
   "metadata": {},
   "source": [
    "Refer to the notebook `3.Evaulate_Llama3.2-1B.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
