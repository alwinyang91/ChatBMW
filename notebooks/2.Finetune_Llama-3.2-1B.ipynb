{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9ec536",
   "metadata": {},
   "source": [
    "# Fine Tune Llama-3.2-1B Model on BMW Press Releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ca56e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "unsloth/Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "print(MODEL_NAME)\n",
    "# Training settings\n",
    "# llama3.2 support 128k tokens\n",
    "MAX_SEQ_LENGTH = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "LOAD_IN_4BIT = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "WANDB_PROJECT = \"BMW-Llama-3.2-1B\"\n",
    "WANDB_ENTITY = None  \n",
    "WANDB_RUN_NAME = \"BMW-Llama-3.2-1B-2000Articles2\"\n",
    "\n",
    "TRAIN_CHAT_DATA_FILE_NAME = '../datasets/chat_data_2000/train_chat.jsonl'\n",
    "VAL_CHAT_DATA_FILE_NAME = '../datasets/chat_data_2000/val_chat.jsonl'\n",
    "TEST_CHAT_DATA_FILE_NAME = '../datasets/chat_data_2000/test_chat.jsonl'\n",
    "\n",
    "# Set checkpoint directory\n",
    "CHECKPOINT_DIR = f\"../{WANDB_RUN_NAME}\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Use the checkpoint_dir variable to save models\n",
    "LORA_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"lora_model\")\n",
    "MERGED_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"merged_model\")\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(LORA_MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(MERGED_MODEL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df016e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace Chat Datasets:\n",
      "  Train: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 6264\n",
      "})\n",
      "  Validation: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 787\n",
      "})\n",
      "  Test: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 787\n",
      "})\n",
      "\n",
      "Dataset features: {'messages': List({'content': Value('string'), 'role': Value('string')})}\n",
      "\n",
      "First sample structure:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are an expert at summarizing BMW news articles. Provide concise, informative summaries that capture the key points.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Summarize the following BMW news article in a concise way.\\n\\n‚ÄúWe are delighted to be joining BMW M in celebrating the 25th anniversary season of our partnership in 2023,‚Äù said Carmelo Ezpeleta, CEO of Dorna Sports. ‚ÄúIn BMW M we have had a strong partner at our side for a quarter of a century; a partner with whom we have enjoyed superb collaboration in many different areas. We are very proud of this long-standing partnership that is never at a standstill, it gives plenty of fresh momentum each year. With the most innovative technologies, BMW M is taking care of safety in our sport for the 25th year now, and is a firm fixture in the MotoGP paddock with a wide range of activities. Here‚Äôs to a fantastic anniversary season in 2023!‚Äù\\n\\n‚Äú2023 is our 25th season as Official Car of MotoGP ‚Äì a long-standing, close partnership like this is special in international racing,‚Äù added Franciscus van Meel, CEO of BMW M GmbH. ‚ÄúWe would like to thank Dorna Sports for the great trust they have placed in us for a quarter of a century now. Since 1999 we have been striving to incorporate our values such as innovation, technology, emotion, and passion into our partnership with Dorna Sports as well. Our new BMW M2 MotoGP safety car is just one example of this. With it we are continuing our tradition of putting the latest BMW M high-performance automobiles at the service of MotoGP safety. Be it the safety car fleet, the BMW M Award, or our many other activities in the top level of motorcycle racing ‚Äì we are proud that Fascination M has become an important part of the MotoGP.‚Äù\\n\\nThe new BMW M2 impresses with the traditional characteristics of a high-performance sports car ‚Äì in a concentrated form and with state-of-the-art technology. With its compact dimensions and the 460 hp TwinPower turbo in-line 6-cylinder engine, the new BMW M2 is the perfect basis for a BMW M safety car. The BMW M2 was kitted out with various safety car elements for its deployment on MotoGP‚Ñ¢ racetracks. These elements include a roll bar, RECARO seats, 6-point racing harness, a fire extinguisher, the safety car roof bar, and the matching front light. Several M Performance parts, such as exhaust system, chassis, carbon wing mirror covers, diffusor, and rear wing have also been installed. The BMW M2 MotoGP‚Ñ¢ safety car will have the standard BMW M safety car livery. In addition to the new BMW M2 MotoGP‚Ñ¢ safety car, the safety car fleet of 2023 also includes many other BMW M safety, medical and official cars, as well as BMW M 1000 RR safety bikes.\\n\\nFor more information about BMW M GmbH and the involvement as the ‚ÄòOfficial Car of MotoGP‚Ñ¢‚Äô, see: \\n\\nCO2 Emissions & Consumption: BMW M2: combined fuel consumption in accordance with WLTP: 9.8 ‚Äì 9.6 l/100 km; combined CO2 emissions in accordance with WLTP: 222 ‚Äì 218 g/km; details as per NEFZ: ‚Äì',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The Grande Pr√©mio de Portugal from 24th to 26th March at Portim√£o marks the start of an anniversary season for BMW M, the 25th season as Official Car of MotoGP‚Ñ¢. The close partnership with MotoGP‚Ñ¢ rights holder Dorna Sports dates back to 1999. The start of the anniversary season in Portugal will see the new BMW M safety car make its debut. The new BMW M2 MotoGP‚Ñ¢ safety car is based on the new BMW M2, which made its world premiere in October.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = load_dataset(\"json\", data_files=TRAIN_CHAT_DATA_FILE_NAME, split=\"train\")\n",
    "dataset_val = load_dataset(\"json\", data_files=VAL_CHAT_DATA_FILE_NAME, split=\"train\")\n",
    "dataset_test = load_dataset(\"json\", data_files=TEST_CHAT_DATA_FILE_NAME, split=\"train\")\n",
    "\n",
    "train_ds = Dataset.from_list(dataset_train)\n",
    "val_ds = Dataset.from_list(dataset_val)\n",
    "test_ds = Dataset.from_list(dataset_test)\n",
    "\n",
    "print(\"HuggingFace Chat Datasets:\")\n",
    "print(f\"  Train: {train_ds}\")\n",
    "print(f\"  Validation: {val_ds}\")\n",
    "print(f\"  Test: {test_ds}\")\n",
    "\n",
    "print(f\"\\nDataset features: {train_ds.features}\")\n",
    "print(f\"\\nFirst sample structure:\")\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb741a",
   "metadata": {},
   "source": [
    "## Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad362506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5090. Num GPUs = 1. Max memory: 31.348 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME, \n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = LOAD_IN_4BIT,\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb87fda",
   "metadata": {},
   "source": [
    "### Define format function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f556e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Use llama-3.2 chat template for Llama-3.2 Instruct models\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.2\",  # Use conversational format for Instruct models\n",
    ")\n",
    "\n",
    "def formatting_chat_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Format chat conversations for training.\n",
    "    Input: examples with 'messages' field containing list of {role, content} dicts\n",
    "    Output: formatted text strings with chat template applied\n",
    "    \"\"\"\n",
    "    messages_list = examples[\"messages\"]\n",
    "    \n",
    "    texts = []\n",
    "    for messages in messages_list:\n",
    "        # Apply chat template to the messages\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize = False,\n",
    "            add_generation_prompt = False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887afb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469fb0bb74824c689fbb1d0f61251873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting train dataset:   0%|          | 0/6264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681f511ad9fc46489621bd0e5849bc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting validation dataset:   0%|          | 0/787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Chat template applied to datasets\n",
      "Train formatted: 6264 samples\n",
      "Validation formatted: 787 samples\n"
     ]
    }
   ],
   "source": [
    "# Apply chat template formatting to create 'text' field for training\n",
    "train_formatted_ds = train_ds.map(\n",
    "    formatting_chat_prompts_func,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    desc=\"Formatting train dataset\"\n",
    ")\n",
    "val_formatted_ds = val_ds.map(\n",
    "    formatting_chat_prompts_func,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    desc=\"Formatting validation dataset\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Chat template applied to datasets\")\n",
    "print(f\"Train formatted: {len(train_formatted_ds)} samples\")\n",
    "print(f\"Validation formatted: {len(val_formatted_ds)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f678584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Example formatted text (with Llama-3.2 chat template):\n",
      "============================================================\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "You are an expert at summarizing BMW news articles. Provide concise, informative summaries that capture the key points.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Summarize the following BMW news article in a concise way.\n",
      "\n",
      "‚ÄúWe are delighted to be joining BMW M in celebrating the 25th anniversary season of our partnership in 2023,‚Äù said Carmelo Ezpeleta, CEO of Dorna Sports. ‚ÄúIn BMW M we have had a strong partner at our side for a quarter of a century; a partner with whom we have enjoyed superb collaboration in many different areas. We are very proud of this long-standing partnership that is never at a standstill, it gives plenty of fresh momentum each year. With the most innovative technologies, BMW M is taking care of safety in our sport for the 25th year now, and is a firm fixture in the MotoGP paddock with a wide range of activities. Here‚Äôs to a fantastic anniversary season in 2023!‚Äù\n",
      "\n",
      "‚Äú2023 is our 25th season as Official Car of MotoGP ‚Äì a long-standing, close partnership like this is special in international racing,‚Äù added Franciscus van Meel, CEO of BMW M GmbH. ‚ÄúWe would like to thank Dorna Sports for the great trust they have placed in us for a quarter of a century now. Since 1999 we have been striving to incorporate our values such as innovation, technology, emotion, and passion into our partnership with Dorna Sports as well. Our new BMW M2 MotoGP safety car is just one example of this. With it we are continuing our tradition of putting the latest BMW M high-performance automobiles at the service of MotoGP safety. Be it the safety car fleet, the BMW M Award, or our many other activities in the top level of motorcycle racing ‚Äì we are proud that Fascination M has become an important part of the MotoGP.‚Äù\n",
      "\n",
      "The new BMW M2 impresses with the traditional characteristics of a high-performance sports car ‚Äì in a concentrated form and with \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Show example of formatted text with chat template applied\n",
    "print(\"=\" * 60)\n",
    "print(\"Example formatted text (with Llama-3.2 chat template):\")\n",
    "print(\"=\" * 60)\n",
    "print(train_formatted_ds[0]['text'][:2000])\n",
    "print(\"...\" if len(train_formatted_ds[0]['text']) > 2000 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3cf5c1",
   "metadata": {},
   "source": [
    "### Filter long samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a284540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0022f45526e54c898df89a952e0303b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering long train samples:   0%|          | 0/6264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4efa3cc9a8f41cf9577f686244e15ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering long val samples:   0%|          | 0/787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 112 train and 20 val samples (too long, assistant would be truncated)\n"
     ]
    }
   ],
   "source": [
    "instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "response_tokens = tokenizer(response_part, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "def filter_long_samples(example):\n",
    "    \"\"\"Keep only samples where assistant response won't be truncated.\"\"\"\n",
    "    tokens = tokenizer(example[\"text\"], add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(tokens) <= MAX_SEQ_LENGTH:\n",
    "        return True\n",
    "    # Find where assistant response starts\n",
    "    for i in range(len(tokens) - len(response_tokens)):\n",
    "        if tokens[i:i+len(response_tokens)] == response_tokens:\n",
    "            return i < MAX_SEQ_LENGTH  # Keep if assistant starts before truncation\n",
    "    return True  # Keep if no assistant marker found (shouldn't happen)\n",
    "\n",
    "train_before = len(train_formatted_ds)\n",
    "val_before = len(val_formatted_ds)\n",
    "\n",
    "train_formatted_ds = train_formatted_ds.filter(filter_long_samples, desc=\"Filtering long train samples\")\n",
    "val_formatted_ds = val_formatted_ds.filter(filter_long_samples, desc=\"Filtering long val samples\")\n",
    "\n",
    "train_filtered = train_before - len(train_formatted_ds)\n",
    "val_filtered = val_before - len(val_formatted_ds)\n",
    "if train_filtered > 0 or val_filtered > 0:\n",
    "    print(f\"Filtered out {train_filtered} train and {val_filtered} val samples (too long, assistant would be truncated)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92102047",
   "metadata": {},
   "source": [
    "# Train\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f450c",
   "metadata": {},
   "source": [
    "### Add LoRA adapters to fine tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4480ecd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2026.1.2 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import weave\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load WANDB_API_KEY from .env file\n",
    "\n",
    "wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            name=WANDB_RUN_NAME,\n",
    "            entity=WANDB_ENTITY,  # None Ë°®Á§∫‰ΩøÁî®ÈªòËÆ§Ë¥¶Êà∑\n",
    "            config={\n",
    "                \"model\": \"Llama-3.2-1B-Instruct\",\n",
    "                \"task\": \"BMW News Fine-tuning\",\n",
    "                \"method\": \"LoRA\",\n",
    "                \"dataset\": \"bmw_training_latest.json\",\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27c27f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d73adb8fa214d72b1098790f76951a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/6152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd6d884cd464b74b2397038e7d918ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì SFTTrainer configured\n",
      "  Train samples: 6152\n",
      "  Validation samples: 767\n"
     ]
    }
   ],
   "source": [
    "# Use the formatted chat datasets (train_formatted and val_formatted)\n",
    "# These have been converted to chat format using chatbmw processor\n",
    "# and formatted with Llama-3.2 chat template\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_formatted_ds,      # Chat format dataset with 'text' field\n",
    "    eval_dataset = val_formatted_ds,          # Validation dataset\n",
    "    dataset_text_field = \"text\",           # The field containing formatted text\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing = False,  # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 8,\n",
    "        per_device_eval_batch_size = 4,    \n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 50,                   # Evaluate less frequently due to more samples\n",
    "        gradient_accumulation_steps = 1,\n",
    "        warmup_steps = 10,\n",
    "        num_train_epochs = 20,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 10,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.001,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"wandb\",\n",
    "        # Checkpoint saving configuration\n",
    "        output_dir = CHECKPOINT_DIR,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 50,\n",
    "        save_total_limit = 3,  \n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(f\"‚úì SFTTrainer configured\")\n",
    "print(f\"  Train samples: {len(train_formatted_ds)}\")\n",
    "print(f\"  Validation samples: {len(val_formatted_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82be365b",
   "metadata": {},
   "source": [
    "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9098ef77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747c7f0e28ea4ef49cff6e40ef591b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=36):   0%|          | 0/6152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd4d033e7034cf581631e71b3a7c0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=36):   0%|          | 0/767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeccf592",
   "metadata": {},
   "source": [
    "We verify masking is actually done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df0ed319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are an expert at creating headlines for BMW news articles. Generate concise, informative, and engaging titles.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGenerate a concise and informative title for the following BMW news article.\\n\\n+++ Call for Metaverse solutions to solve specific industrial challenges +++ Opportunity of a partnership with the BMW Group for the winning teams +++Munich. The BMW Group Supplierthon with a focus on the ‚ÄúMetaverse and other Virtual Experiences‚Äù is starting today. The aim is to attract researchers, startups and pioneering tech leaders from within the global Metaverse community in order to gain an outside-in perspective. Application here: Link to Metaverse Supplierthon The Metaverse is the next iteration of the internet: a single, shared, persistent, immersive, 3D virtual space where humans experience life in ways they could not in the physical world. The Supplierthon will empower applicants to bring their innovative and visionary ideas for the Metaverse to life. Experts from the BMW Group have identified challenges in three different areas (for details see below): Vehicle Readiness, In-Car Experience and Virtual Ecosystems. The BMW Group believes that virtual experiences could make a significant difference by strengthening the entire customer experience and the product substance and could have an impact on the whole automotive industry in the future. Registration begins today and the first deadline for submission is March 15, 2023. The ideas that are submitted will be examined and judged by a panel of experts who will select the finalists. They will proceed to the second round, where the ideas will be developed further to produce real business solutions by May 10, 2023. The second round will end with a pitch event where the finalists will present their solutions to a panel of experts before the winners are selected on May 24, 2023. The winners will have the opportunity to partner with the BMW Group and will also be involved in the implementation of pilot projects. The BMW Group is a pioneer when it comes to virtualization and is now taking one step further The Supplierthon for the ‚ÄúMetaverse and other Virtual Experiences‚Äù highlights the fact that the BMW Group is playing a leading role in efforts to bring the next level of digitalized human interaction to life. The BMW Group is generating value by consistently developing greater interaction and more immersive experiences using XR1 and Web 3.0, building on a vivid landscape of use cases, some of which are already connected, along the entire value chain. To ensure that its solution is coherent, the BMW Group is currently developing a strategic, centralized Metaverse Platform. In December 2022, the company became one of the first automotive manufacturers to join the Metaverse Standards Forum. The company takes a holistic approach across three strategic dimensions: a Corporate Metaverse, a Commercial Metaverse and an In-Car Metaverse, as recently demonstrated by the BMW i Vision Dee vehicle and its Mixed Reality Slider, which was presented at the CES in Las Vegas. The Metaverse Supplierthon will focus only on in-car use cases. ‚ÄúFrom our perspective, our industry is in the process of a step by step transformation, shifting human experiences into a virtual world driven by the metaverse.‚Äù says Rudi Bencker, Senior Vice President Global Research, Tech Offices and Development Cooperations. ‚ÄúWith this Crowd Innovation Challenge we want to take the next step and invite the global metaverse community to share their innovative and visionary approaches within the Metaverse spectrum.‚Äù says Florian Weig, Senior Vice President Purchasing and Supplier Network Digital. Specific Challenges for the Metaverse Supplierthon\\n\\n1Extended Reality (XR): Multisensory extended reality integrates Mixed Reality (MR), Virtual Reality (VR), Augmented Reality (AR) and the five traditional senses, including sight, hearing, smell, taste and touch. 2 Augmented Reality (AR) describes the computer-aided extension of reality perception. 3 Virtual Reality (VR) is the representation and simultaneous perception of an apparent reality and its physical properties in a real-time computer-generated, interactive virtual environment. 4 Mixed Reality (MR) comprises environments or systems that mix natural and artificial perception.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nLaunch of a Supplierthon for the ‚ÄûMetaverse and other Virtual Experiences‚Äù to crowd-source Innovation ‚Äì apply now!<|eot_id|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d192069c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Launch of a Supplierthon for the ‚ÄûMetaverse and other Virtual Experiences‚Äù to crowd-source Innovation ‚Äì apply now!<|eot_id|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e557e",
   "metadata": {},
   "source": [
    "We can see the System and Instruction prompts are successfully masked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11ad01b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 5090. Max memory = 31.348 GB.\n",
      "2.41 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78bc2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience = 10,     # How many steps we will wait if the eval loss doesn't decrease\n",
    "                                     # For example the loss might increase, but decrease after 3 steps\n",
    "    early_stopping_threshold = 0.0,  # Can set higher - sets how much loss should decrease by until\n",
    "                                     # we consider early stopping. For eg 0.01 means if loss was\n",
    "                                     # 0.02 then 0.01, we consider to early stop the run.\n",
    ")\n",
    "trainer.add_callback(early_stopping_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98e6de",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9529c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/zewen/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Copying 1 files from cache to `../bmw-llama-3.2-1b-1500articles/merged_model`: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully copied all 1 files from cache to `../bmw-llama-3.2-1b-1500articles/merged_model`\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 29330.80it/s]\n",
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/zewen/alwinyang91/ChatBMW/bmw-llama-3.2-1b-1500articles/merged_model`\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA model\n",
    "model.save_pretrained(LORA_MODEL_PATH)\n",
    "tokenizer.save_pretrained(LORA_MODEL_PATH)\n",
    "\n",
    "# Merge LoRA weights into base model and save\n",
    "model.save_pretrained_merged(\n",
    "    MERGED_MODEL_PATH,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",  # Options: \"merged_16bit\", \"merged_4bit\", \"lora\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
