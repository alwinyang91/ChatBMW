{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63008278",
   "metadata": {},
   "source": [
    "# Fine-Tune A Reduced LLAMA3.2-1B Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ca56e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsloth/Llama-3.2-1B-Instruct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import DataCollatorForSeq2Seq, EarlyStoppingCallback, AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig\n",
    "\n",
    "MODEL_NAME = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "print(MODEL_NAME)\n",
    "# Training settings\n",
    "# llama3.2 support 128k tokens\n",
    "MAX_SEQ_LENGTH = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "LOAD_IN_4BIT = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "WANDB_PROJECT = \"BMW-Llama-3.2-1B\"\n",
    "WANDB_ENTITY = None  \n",
    "WANDB_RUN_NAME = \"BMW-Llama-3.2-1B-2000Articles_pruned\"\n",
    "\n",
    "TRAIN_CHAT_DATA_FILE_NAME = '../datasets/chat_data_2000/train_chat.jsonl'\n",
    "VAL_CHAT_DATA_FILE_NAME = '../datasets/chat_data_2000/val_chat.jsonl'\n",
    "TEST_CHAT_DATA_FILE_NAME = '../datasets/chat_data_2000/test_chat.jsonl'\n",
    "\n",
    "# Set checkpoint directory\n",
    "CHECKPOINT_DIR = f\"../{WANDB_RUN_NAME}\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Use the checkpoint_dir variable to save models\n",
    "LORA_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"lora_model\")\n",
    "MERGED_MODEL_PATH = os.path.join(CHECKPOINT_DIR, \"merged_model\")\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(LORA_MODEL_PATH, exist_ok=True)\n",
    "os.makedirs(MERGED_MODEL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb741a",
   "metadata": {},
   "source": [
    "## Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad362506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Structures the dataset into prompt-expected output pairs.\n",
    "def formatting_function(examples):\n",
    "    return tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False, add_generation_prompt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06591a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original layer count: 16\n"
     ]
    }
   ],
   "source": [
    "layers = model.model.layers\n",
    "print(f\"Original layer count: {len(layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f7c1d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 15\n",
      "Config num_hidden_layers: 15\n",
      "\n",
      "Layer structure:\n",
      "  Layer 0: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 1: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 2: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 3: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 4: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 5: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 6: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 7: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 8: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 9: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 10: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 11: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 12: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 13: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "  Layer 14: LlamaDecoderLayer\n",
      "    - self_attn: LlamaAttention\n",
      "    - mlp: LlamaMLP\n",
      "    - input_layernorm: LlamaRMSNorm\n",
      "    - post_attention_layernorm: LlamaRMSNorm\n",
      "\n",
      "Detailed info for first layer (Layer 0):\n",
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.model.layers = torch.nn.ModuleList(layers[:-1])\n",
    "model.config.num_hidden_layers = len(model.model.layers)\n",
    "\n",
    "# Print layer information\n",
    "print(f\"Number of layers: {len(model.model.layers)}\")\n",
    "print(f\"Config num_hidden_layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"\\nLayer structure:\")\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    print(f\"  Layer {i}: {type(layer).__name__}\")\n",
    "    if hasattr(layer, 'self_attn'):\n",
    "        print(f\"    - self_attn: {type(layer.self_attn).__name__}\")\n",
    "    if hasattr(layer, 'mlp'):\n",
    "        print(f\"    - mlp: {type(layer.mlp).__name__}\")\n",
    "    if hasattr(layer, 'input_layernorm'):\n",
    "        print(f\"    - input_layernorm: {type(layer.input_layernorm).__name__}\")\n",
    "    if hasattr(layer, 'post_attention_layernorm'):\n",
    "        print(f\"    - post_attention_layernorm: {type(layer.post_attention_layernorm).__name__}\")\n",
    "\n",
    "# Print detailed info for first layer as example\n",
    "if len(model.model.layers) > 0:\n",
    "    print(f\"\\nDetailed info for first layer (Layer 0):\")\n",
    "    print(model.model.layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de9af398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New layer count: 15\n",
      "Config num_layers: 15\n"
     ]
    }
   ],
   "source": [
    "print(f\"New layer count: {len(model.model.layers)}\")\n",
    "print(f\"Config num_layers: {model.config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fafbfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2fedc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace Chat Datasets:\n",
      "  Train: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 6264\n",
      "})\n",
      "  Validation: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 787\n",
      "})\n",
      "  Test: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 787\n",
      "})\n",
      "\n",
      "Dataset features: {'messages': List({'content': Value('string'), 'role': Value('string')})}\n",
      "\n",
      "First sample structure:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are an expert at summarizing BMW news articles. Provide concise, informative summaries that capture the key points.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Summarize the following BMW news article in a concise way.\\n\\n“We are delighted to be joining BMW M in celebrating the 25th anniversary season of our partnership in 2023,” said Carmelo Ezpeleta, CEO of Dorna Sports. “In BMW M we have had a strong partner at our side for a quarter of a century; a partner with whom we have enjoyed superb collaboration in many different areas. We are very proud of this long-standing partnership that is never at a standstill, it gives plenty of fresh momentum each year. With the most innovative technologies, BMW M is taking care of safety in our sport for the 25th year now, and is a firm fixture in the MotoGP paddock with a wide range of activities. Here’s to a fantastic anniversary season in 2023!”\\n\\n“2023 is our 25th season as Official Car of MotoGP – a long-standing, close partnership like this is special in international racing,” added Franciscus van Meel, CEO of BMW M GmbH. “We would like to thank Dorna Sports for the great trust they have placed in us for a quarter of a century now. Since 1999 we have been striving to incorporate our values such as innovation, technology, emotion, and passion into our partnership with Dorna Sports as well. Our new BMW M2 MotoGP safety car is just one example of this. With it we are continuing our tradition of putting the latest BMW M high-performance automobiles at the service of MotoGP safety. Be it the safety car fleet, the BMW M Award, or our many other activities in the top level of motorcycle racing – we are proud that Fascination M has become an important part of the MotoGP.”\\n\\nThe new BMW M2 impresses with the traditional characteristics of a high-performance sports car – in a concentrated form and with state-of-the-art technology. With its compact dimensions and the 460 hp TwinPower turbo in-line 6-cylinder engine, the new BMW M2 is the perfect basis for a BMW M safety car. The BMW M2 was kitted out with various safety car elements for its deployment on MotoGP™ racetracks. These elements include a roll bar, RECARO seats, 6-point racing harness, a fire extinguisher, the safety car roof bar, and the matching front light. Several M Performance parts, such as exhaust system, chassis, carbon wing mirror covers, diffusor, and rear wing have also been installed. The BMW M2 MotoGP™ safety car will have the standard BMW M safety car livery. In addition to the new BMW M2 MotoGP™ safety car, the safety car fleet of 2023 also includes many other BMW M safety, medical and official cars, as well as BMW M 1000 RR safety bikes.\\n\\nFor more information about BMW M GmbH and the involvement as the ‘Official Car of MotoGP™’, see: \\n\\nCO2 Emissions & Consumption: BMW M2: combined fuel consumption in accordance with WLTP: 9.8 – 9.6 l/100 km; combined CO2 emissions in accordance with WLTP: 222 – 218 g/km; details as per NEFZ: –',\n",
       "   'role': 'user'},\n",
       "  {'content': 'The Grande Prémio de Portugal from 24th to 26th March at Portimão marks the start of an anniversary season for BMW M, the 25th season as Official Car of MotoGP™. The close partnership with MotoGP™ rights holder Dorna Sports dates back to 1999. The start of the anniversary season in Portugal will see the new BMW M safety car make its debut. The new BMW M2 MotoGP™ safety car is based on the new BMW M2, which made its world premiere in October.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = load_dataset(\"json\", data_files=TRAIN_CHAT_DATA_FILE_NAME, split=\"train\")\n",
    "dataset_val = load_dataset(\"json\", data_files=VAL_CHAT_DATA_FILE_NAME, split=\"train\")\n",
    "dataset_test = load_dataset(\"json\", data_files=TEST_CHAT_DATA_FILE_NAME, split=\"train\")\n",
    "\n",
    "# =============================================================================\n",
    "# Step 3: Create HuggingFace Datasets from chat data\n",
    "# =============================================================================\n",
    "train_ds = Dataset.from_list(dataset_train)\n",
    "val_ds = Dataset.from_list(dataset_val)\n",
    "test_ds = Dataset.from_list(dataset_test)\n",
    "\n",
    "print(\"HuggingFace Chat Datasets:\")\n",
    "print(f\"  Train: {train_ds}\")\n",
    "print(f\"  Validation: {val_ds}\")\n",
    "print(f\"  Test: {test_ds}\")\n",
    "\n",
    "print(f\"\\nDataset features: {train_ds.features}\")\n",
    "print(f\"\\nFirst sample structure:\")\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92102047",
   "metadata": {},
   "source": [
    "# Train\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f450c",
   "metadata": {},
   "source": [
    "### Add LoRA adapters to fine tune "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import weave\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load WANDB_API_KEY from .env file\n",
    "\n",
    "wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            name=WANDB_RUN_NAME,\n",
    "            entity=WANDB_ENTITY,  # None 表示使用默认账户\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c27f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786ca22481a441a680eb6eca329f376c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/6264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1867b4753254c3ea4696c6b3167a8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/6264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0bbac87a4a4c2796fb38e69d2e2f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/6264 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1f8470dfba4d619e5b8629bc5040e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7d6ff4efd248e29a6a8986acdcb35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2d31984a094560b789bd38a9ceacd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "     processing_class=tokenizer,\n",
    "    train_dataset=train_ds,           # Use the original dataset with \"messages\" field\n",
    "    eval_dataset=val_ds,\n",
    "    formatting_func=formatting_function,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=1,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        gradient_accumulation_steps=1,\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=20,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.001,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"wandb\",\n",
    "        output_dir=CHECKPOINT_DIR,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11ad01b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 5090. Max memory = 31.348 GB.\n",
      "4.746 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78bc2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience = 20,     # How many steps we will wait if the eval loss doesn't decrease\n",
    "                                     # For example the loss might increase, but decrease after 3 steps\n",
    "    early_stopping_threshold = 0.0,  # Can set higher - sets how much loss should decrease by until\n",
    "                                     # we consider early stopping. For eg 0.01 means if loss was\n",
    "                                     # 0.02 then 0.01, we consider to early stop the run.\n",
    ")\n",
    "trainer.add_callback(early_stopping_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe86c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98e6de",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA model (trainer.model already contains the trained LoRA weights)\n",
    "trainer.model.save_pretrained(LORA_MODEL_PATH)\n",
    "tokenizer.save_pretrained(LORA_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    dtype=torch.bfloat16, \n",
    "    device_map='auto'\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Ensure pad token is set (must match training configuration)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load trained LoRA adapters from the training checkpoint\n",
    "model_with_lora = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    '../bmw-llama-3.2-1b-2000articles_stretch/checkpoint-2750',\n",
    "    )\n",
    "\n",
    "# Merge and use for inference\n",
    "merged_model = model_with_lora.merge_and_unload()\n",
    "merged_model.eval()\n",
    "\n",
    "merged_model.save_pretrained(MERGED_MODEL_PATH, safe_serialization=True)\n",
    "tokenizer.save_pretrained(MERGED_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
